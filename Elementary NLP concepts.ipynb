{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This talk covers\n",
    "* Term Frequency (i.e. vectorized text)\n",
    "* Stop words\n",
    "* Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "* Negation Handling\n",
    "* Character NGrams\n",
    "\n",
    "and includes a demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency (i.e. vectorized text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NLP/term-frequency.jpg\" style=\"height: 250px;\"/>\n",
    "_From: https://www.linkedin.com/pulse/term-frequency-inverse-document-sanjay-singh-1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I liked the movie, just not the popcorn.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>just</th>\n",
       "      <th>liked</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>popcorn</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   just  liked  movie  not  popcorn  the\n",
       "0   1.0    1.0    1.0  1.0      1.0  2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=False, norm=None)\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "pd.DataFrame(vectorizer.transform(docs).toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Note: \"I\" is missing because the default tokenizer omits one letter words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NLP/stop-words.png\" style=\"height: 200px;\"/>\n",
    "_From: https://www.slideshare.net/tomeksobczak/stopwords-in-search_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>just</th>\n",
       "      <th>liked</th>\n",
       "      <th>movie</th>\n",
       "      <th>popcorn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   just  liked  movie  popcorn\n",
       "0   1.0    1.0    1.0      1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=False, norm=None, stop_words='english')\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "pd.DataFrame(vectorizer.transform(docs).toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Note \"the\" and \"not\" considered stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NLP/tf-idf.png\" style=\"height: 300px;\"/>\n",
    "_Based on image from: http://ccdoc-tecnicasrecuperacioninformacion.blogspot.com/2012/11/frecuencias-y-pesos-de-los-terminos-de.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NLP/tfidf-downward.jpg\" style=\"height: 400px;\"/>\n",
    "_From: http://seocopywriting.com/tf-idf-killed-copywriting-spam/_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I liked the movie, just not the popcorn.\",\n",
    "    \"I liked the song, just not the words.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>just</th>\n",
       "      <th>liked</th>\n",
       "      <th>movie</th>\n",
       "      <th>popcorn</th>\n",
       "      <th>song</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   just  liked  movie  popcorn  song  words\n",
       "0   1.0    1.0    1.0      1.0   0.0    0.0\n",
       "1   1.0    1.0    0.0      0.0   1.0    1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=False, norm=None, stop_words='english')\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "pd.DataFrame(vectorizer.transform(docs).toarray(),\n",
    "             columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Note: \"I\" is missing because the default tokenizer omits one letter words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>just</th>\n",
       "      <th>liked</th>\n",
       "      <th>movie</th>\n",
       "      <th>popcorn</th>\n",
       "      <th>song</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   just  liked     movie   popcorn      song     words\n",
       "0   1.0    1.0  1.405465  1.405465  0.000000  0.000000\n",
       "1   1.0    1.0  0.000000  0.000000  1.405465  1.405465"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, norm=None, stop_words='english')\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "pd.DataFrame(vectorizer.transform(docs).toarray(),\n",
    "             columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Note: \"I\" is missing because the default tokenizer omits one letter words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NLP/negation-handling.png\" style=\"height: 100px;\"/>\n",
    "_all my own artwork :P_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I liked the movie, just not the popcorn.\",\n",
    "    \"I liked the popcorn, just not the movie.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without negation handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>just</th>\n",
       "      <th>liked</th>\n",
       "      <th>movie</th>\n",
       "      <th>popcorn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   just  liked  movie  popcorn\n",
       "0   1.0    1.0    1.0      1.0\n",
       "1   1.0    1.0    1.0      1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=False, norm=None,\n",
    "                             stop_words='english') # Note: no TF-IDF\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "pd.DataFrame(vectorizer.transform(docs).toarray(),\n",
    "             columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With negation handlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I liked the movie , just not the_NEG popcorn_NEG .',\n",
       " 'I liked the popcorn , just not the_NEG movie_NEG .']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "negation_marked_docs = [' '.join(mark_negation(word_tokenize(doc)))\n",
    "                            for doc in docs]\n",
    "negation_marked_docs\n",
    "\n",
    "# Note \"the_NEG\" which is here because this hack doesn't account for stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>just</th>\n",
       "      <th>liked</th>\n",
       "      <th>movie</th>\n",
       "      <th>movie_neg</th>\n",
       "      <th>popcorn</th>\n",
       "      <th>popcorn_neg</th>\n",
       "      <th>the_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   just  liked  movie  movie_neg  popcorn  popcorn_neg  the_neg\n",
       "0   1.0    1.0    1.0        0.0      0.0          1.0      1.0\n",
       "1   1.0    1.0    0.0        1.0      1.0          0.0      1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=False, norm=None,\n",
    "                             stop_words='english')\n",
    "vectorizer.fit(negation_marked_docs)\n",
    "\n",
    "pd.DataFrame(vectorizer.transform(negation_marked_docs).toarray(),\n",
    "             columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character NGrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Polina's talk covered Word NGrames)_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of NGrams where N=5:\n",
    "<img src=\"NLP/character-ngrams.jpg\" style=\"height: 300px;\"/>\n",
    "_From: http://plaza.ufl.edu/jgu/public_html/C-uppsats/cup.html_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I liked the movie, just not the popcorn.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u' j',\n",
       " u' l',\n",
       " u' m',\n",
       " u' n',\n",
       " u' p',\n",
       " u' t',\n",
       " u', ',\n",
       " u'co',\n",
       " u'd ',\n",
       " u'e ',\n",
       " u'e,',\n",
       " u'ed',\n",
       " u'he',\n",
       " u'i ',\n",
       " u'ie',\n",
       " u'ik',\n",
       " u'ju',\n",
       " u'ke',\n",
       " u'li',\n",
       " u'mo',\n",
       " u'n.',\n",
       " u'no',\n",
       " u'op',\n",
       " u'or',\n",
       " u'ot',\n",
       " u'ov',\n",
       " u'pc',\n",
       " u'po',\n",
       " u'rn',\n",
       " u'st',\n",
       " u't ',\n",
       " u'th',\n",
       " u'us',\n",
       " u'vi']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=False, norm=None,\n",
    "                             ngram_range=(2,2), analyzer='char')\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>n</th>\n",
       "      <th>p</th>\n",
       "      <th>t</th>\n",
       "      <th>,</th>\n",
       "      <th>co</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>...</th>\n",
       "      <th>ot</th>\n",
       "      <th>ov</th>\n",
       "      <th>pc</th>\n",
       "      <th>po</th>\n",
       "      <th>rn</th>\n",
       "      <th>st</th>\n",
       "      <th>t</th>\n",
       "      <th>th</th>\n",
       "      <th>us</th>\n",
       "      <th>vi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     j    l    m    n    p    t   ,    co   d    e  ...    ot   ov   pc   po  \\\n",
       "0  1.0  1.0  1.0  1.0  1.0  2.0  1.0  1.0  1.0  2.0 ...   1.0  1.0  1.0  1.0   \n",
       "\n",
       "    rn   st   t    th   us   vi  \n",
       "0  1.0  1.0  2.0  2.0  1.0  1.0  \n",
       "\n",
       "[1 rows x 34 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vectorizer.transform(docs).toarray(),\n",
    "             columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick character ngrams demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting language from character ngrams\n",
    "<img src=\"NLP/languages.png\" style=\"height: 300px;\"/>\n",
    "_From: http://4chanint.wikia.com/wiki/The_Official_/int/_How_to_Learn_A_Foreign_Language_Guide_Wiki_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = load_files('NLP/paragraphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "969\n",
      "['ar', 'de', 'en', 'es', 'fr', 'it', 'ja', 'nl', 'pl', 'pt', 'ru']\n"
     ]
    }
   ],
   "source": [
    "# ~1000 paragrams across 11 languages\n",
    "print len(dataset.data)\n",
    "print dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All of 1, 2, and 3 ngrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char',\n",
    "                             use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', vectorizer),\n",
    "    ('clf', Perceptron()), # Perceptron performs well on this problem\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training set\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "# Predict the outcome on the testing set in a variable named y_predicted\n",
    "y_predicted = clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 78  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 71  0  0  0  0  0  0  0  0]\n",
      " [ 2  0  1 59  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 52  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0 34  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 40  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0 22  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 16  0  1]\n",
      " [ 3  0  0  0  0  0  0  0  0 46  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 36]]\n"
     ]
    }
   ],
   "source": [
    "# Plot the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974056552112\n"
     ]
    }
   ],
   "source": [
    "# Display F1 score\n",
    "print(metrics.f1_score(y_test, y_predicted, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language of \"This is a language detection test.\" is \"fr\"\n",
      "The language of \"Ceci est un test de détection de la langue.\" is \"fr\"\n",
      "The language of \"Dies ist ein Test, um die Sprache zu erkennen.\" is \"de\"\n"
     ]
    }
   ],
   "source": [
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    u'This is a language detection test.',\n",
    "    u'Ceci est un test de d\\xe9tection de la langue.',\n",
    "    u'Dies ist ein Test, um die Sprache zu erkennen.',\n",
    "]\n",
    "predicted = clf.predict(sentences)\n",
    "\n",
    "for s, p in zip(sentences, predicted):\n",
    "    print(u'The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the ngram hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores = pd.DataFrame([], columns=range(1,10+1))\n",
    "\n",
    "for min_n in range(1,10+1):\n",
    "    for max_n in range(min_n, 10+1):\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(min_n, max_n),\n",
    "                                     analyzer='char', use_idf=False)\n",
    "        clf = Pipeline([\n",
    "                ('vec', vectorizer),\n",
    "                ('clf', Perceptron())\n",
    "              ])\n",
    "        clf.fit(docs_train, y_train)\n",
    "        y_predicted = clf.predict(docs_test)\n",
    "        scores.loc[min_n, max_n] = metrics.f1_score(y_test, y_predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='NLP/results.png'>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
